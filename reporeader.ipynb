{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7365006",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/HarikishanReddy2004/GenAI_teams_summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b43678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./cloned_repo'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clone repo\n",
    "from git import Repo\n",
    "import os\n",
    "def clone_repo(url,destination=\"./cloned_repo\"):\n",
    "    if os.path.exists(destination):\n",
    "        os.system(f\"rm -rf {destination}\")\n",
    "    Repo.clone_from(url,destination)\n",
    "    return destination\n",
    "\n",
    "clone_repo(\"https://github.com/HarikishanReddy2004/GenAI_teams_summarizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c50d5512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives full list of code files in the folder\n",
    "def collect_code_files(folder_path,extensions=[\".py\",\".js\",\".ts\"]):\n",
    "    code_files=[]\n",
    "    for root,dirs,files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if any(file.endswith(ext) for ext in extensions):\n",
    "                code_files.append(os.path.join(root,file))\n",
    "    return code_files\n",
    "file_paths=collect_code_files(\"./cloned_repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42fad0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter files based on size \n",
    "#You don’t want to feed all files to model — some are config files, others are trivial.  filtered based on filename and size.\n",
    "def filter_files(file_paths,exclude_keywords=[\"test\",\"config\",\"setup\"],max_size_kb=100):\n",
    "    filtered_files=[]\n",
    "    for file in file_paths:\n",
    "        if any(keyword in file.lower() for keyword in exclude_keywords):\n",
    "            continue\n",
    "        if os.path.getsize(file)/1024 > max_size_kb:\n",
    "            continue\n",
    "        filtered_files.append(file)\n",
    "    return filtered_files\n",
    "filtered_paths=filter_files(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fff4b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'./cloned_repo\\\\main.py': 'import os,re\\nfrom datetime import datetime\\nfrom config import BOARD_ID\\nfrom jira_client import jira_client\\nfrom transcript_processor import transcript_processor\\nfrom summarizer import summarizer\\n\\ndef match_story_ref_to_key(ref, story_keys):\\n    for key in story_keys:\\n        if ref in key:\\n            return key\\n    return None\\n\\ndef replace_summary_with_person(summary, person_name):\\n    return summary.replace(\"**Summary:**\", f\"{person_name}:\")\\n\\n\\ndef main():\\n    with open(\"D:\\\\\\\\teams_genai\\\\\\\\venv\\\\\\\\conversation.txt\", \"r\") as f:\\n        conversation_text = f.read()\\n    print(BOARD_ID)\\n    print(conversation_text)\\n    sprint_id = jira_client.get_active_sprint(BOARD_ID)\\n    print(sprint_id)\\n    if not sprint_id:\\n        print(\"No active sprint found.\")\\n        return\\n\\n    story_map = jira_client.get_issues_in_sprint(sprint_id)\\n    story_keys = list(story_map.keys())\\n    story_values = list(story_map.values())\\n    print(f\"Fetched {len(story_keys)} stories from sprint.\")\\n\\n    # Extract structured updates: {person: {story_id: [comments]}}\\n    updates = transcript_processor.extract_updates(conversation_text)\\n\\n    # Define summarization instruction\\n    summarization_instruction = (\\n        \"Summarize the updates in a clear, concise manner suitable for posting in a Jira comment. \"\\n        \"Make sure comments should not be repetitive and Dont consider unwanted discussions into account such as \\'He is on mute i guess\\' which are not related to the work at all.\"\\n    )\\n\\n    todays_content=[];weekly_content=[]\\n    for person, stories in updates.items():\\n        for story_id, comments in stories.items():\\n            story_key = match_story_ref_to_key(story_id, story_keys)\\n            if not story_key:\\n                print(f\"Story reference {story_id} not found in active sprint.\")\\n                continue\\n\\n            combined_text = \" \".join(comments)\\n            summary = summarizer.generate_summary(combined_text, summarization_instruction)\\n            # summary=replace_summary_with_person(summary, person)\\n            summary=summarizer.generate_summary(str(summary))\\n            print(f\"Posting comment to {story_key}: {summary}\")\\n\\n            status = jira_client.post_comment(story_key, summary)\\n            if status == 201:\\n                print(f\"  Successfully commented on {story_key}\")\\n            else:\\n                print(f\"  Failed to comment on {story_key}\")\\n            # Collect today\\'s content for further processing or output  \\n\\n            todays_content.append(f\"{summary}\")\\n            weekly_content.append(f\"{story_map[story_key]}:{summary}\")\\n# Save daily summary as before\\n    output_dir = os.path.join(os.path.dirname(__file__), \\'.\\', \\'weekly_summary_inputs\\')\\n    os.makedirs(output_dir, exist_ok=True)\\n    date_str = datetime.now().strftime(\\'%Y-%m-%d\\')\\n    filename = f\\'{date_str}_summary.txt\\'\\n    filepath = os.path.join(output_dir, filename)\\n    with open(filepath, \\'w\\', encoding=\\'utf-8\\') as f:\\n        f.write(\\'\\\\n\\'.join(todays_content))\\n    print(f\"All summaries written to {filepath}\")\\n\\n    # Append today\\'s content to the weekly file\\n    weekly_dir = os.path.join(os.path.dirname(__file__), \\'.\\', \\'weekly_data\\')\\n    os.makedirs(weekly_dir, exist_ok=True)\\n    weekly_file = os.path.join(weekly_dir, \\'current_week.txt\\')\\n    with open(weekly_file, \\'w\\', encoding=\\'utf-8\\') as f:\\n        f.write(\\'\\\\n\\'.join(weekly_content))\\n    print(f\"All summaries written to {weekly_file}\")\\n',\n",
       " './cloned_repo\\\\task_scheduler.py': 'import schedule\\nimport time\\nimport datetime\\nfrom weekly_generator import generate_weekly_report  # assumes you created this as discussed\\nfrom main import main  # assumes you have a main function in main.py that processes daily summaries\\ndef weekly_job():\\n    print(f\"[{datetime.datetime.now()}] Running weekly summary generation...\")\\n    generate_weekly_report(\"D:\\\\\\\\teams_genai\\\\\\\\venv\\\\\\\\weekly_data\")\\n\\ndef daily_job():\\n    print(f\"[{datetime.datetime.now()}] Running daily summary generation...\")\\n    main()\\n#sample time\\nschedule.every().day.at(\"18:52\").do(daily_job)\\nprint(\"Scheduler started. Waiting for daily  job  6.52 PM...\")\\n#sample time\\nschedule.every().thursday.at(\"18:53\").do(weekly_job)\\nprint(\"Scheduler started. Waiting for weekly job 6.53 PM...\")\\n\\nwhile True:\\n    schedule.run_pending()\\n    time.sleep(60)  # Check every minute\\n',\n",
       " './cloned_repo\\\\weekly_generator.py': 'import os\\nimport requests\\nfrom datetime import datetime, timedelta\\nfrom config import gemini_api\\n# Set your API key here or via environment variable\\nGEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", gemini_api)\\n\\nGEMINI_API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\\n\\ndef collect_weekly_text(folder_path):\\n    weekly_text = \"\"\\n    # today = datetime.today()\\n\\n    # Scan all .txt files in the folder from the past 7 days\\n    for file_name in os.listdir(folder_path):\\n        if not file_name.endswith(\".txt\"):\\n            continue\\n\\n        with open(os.path.join(folder_path, file_name), \"r\") as file:\\n            weekly_text += file.read().strip() + \"\\\\n\"\\n \\n\\n    return weekly_text.strip()\\n\\ndef summarize_weekly_text(text):\\n    if not text:\\n        return \"No summaries found for the week.\"\\n\\n    headers = {\"Content-Type\": \"application/json\"}\\n    instruction = (\\n        \"Summarize the following weekly team updates max into 3 crisp bullet points. And also mention what Isses faced under issue heading\"\\n        \"Do not include personal names, PRs, or story IDs. Focus only on key accomplishments,completed tasks, integrations, or validations. \"\\n        \"frame summary points which should be very generic it should be like--> I finished this or that.It should basically give the headings which we focused whole week .\"\\n        \"Ensure this update report will be monitored by the leadership team .so make sure it shouldn\\'t contain non-work related and also basic works.It shoudl cover all week files into few points \"\\n        \"No * marks present, and also there shouldn\\'t be the natural language statements such as here is the updated summary/ whaterver u try to explain.\"\\n    )\\n\\n    payload = {\\n        \"contents\": [{\\n            \"parts\": [{\"text\": f\"{instruction}\\\\n\\\\n{text}\"}]\\n        }],\\n        \"generationConfig\": {\\n            \"temperature\": 0.4,\\n            \"topK\": 40,\\n            \"topP\": 0.95,\\n            \"maxOutputTokens\": 700\\n        }\\n    }\\n\\n    response = requests.post(\\n        f\"{GEMINI_API_URL}?key={GEMINI_API_KEY}\",\\n        headers=headers,\\n        json=payload\\n    )\\n\\n    if response.status_code == 200:\\n        data = response.json()\\n        return data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\\n    else:\\n        return f\"Error: {response.status_code} - {response.text}\"\\n\\ndef generate_weekly_report(folder_path):\\n    print(f\"Collecting summaries from: {folder_path}\")\\n    raw_text = collect_weekly_text(folder_path)\\n    if not raw_text:\\n        print(\"No data collected.\")\\n        return\\n\\n    print(\"Generating weekly summary...\\\\n\")\\n    summary = summarize_weekly_text(raw_text)\\n    print(summary)\\n    report_dir = \"D:\\\\\\\\teams_genai\\\\\\\\venv\\\\\\\\weekly_report\"\\n    weekly_file = os.path.join(report_dir, \\'current_week_report.txt\\')\\n    with open(weekly_file, \\'w\\', encoding=\\'utf-8\\') as f:\\n        f.write(summary)\\n    print(f\"All summaries written to {weekly_file}\")\\n',\n",
       " './cloned_repo\\\\jira_client\\\\jira_client.py': '# jira_client/jira_client.py\\n\\nimport requests\\nfrom requests.auth import HTTPBasicAuth\\nfrom config import JIRA_BASE_URL, JIRA_EMAIL, JIRA_API_TOKEN\\n\\nheaders = {\\n    \"Accept\": \"application/json\",\\n    \"Content-Type\": \"application/json\",\\n}\\n\\nauth = HTTPBasicAuth(JIRA_EMAIL, JIRA_API_TOKEN)\\n\\ndef get_active_sprint(board_id):\\n    url = f\"{JIRA_BASE_URL}/rest/agile/1.0/board/{board_id}/sprint?state=active\"\\n    response = requests.get(url,auth=auth)\\n    if response.status_code == 200:\\n        sprints = response.json().get(\\'values\\', [])\\n        if sprints:\\n            return sprints[0][\\'id\\']\\n    return None\\n\\ndef get_issues_in_sprint(sprint_id):\\n    url = f\"{JIRA_BASE_URL}/rest/agile/1.0/sprint/{sprint_id}/issue\"\\n    response = requests.get(url, headers=headers, auth=auth)\\n    issues = {}\\n    if response.status_code == 200:\\n        for issue in response.json().get(\\'issues\\', []):\\n            issues[issue[\\'key\\']] = issue[\\'fields\\'][\\'summary\\']\\n    return issues\\n\\n\\n\\n\\ndef post_comment(issue_key, comment_body):\\n    url = f\"{JIRA_BASE_URL}/rest/api/3/issue/{issue_key}/comment\"\\n\\n    # Prepare the payload using Jira\\'s Atlassian Document Format (ADF)\\n    payload = {\\n        \"body\": {\\n            \"type\": \"doc\",\\n            \"version\": 1,\\n            \"content\": [\\n                {\\n                    \"type\": \"paragraph\",\\n                    \"content\": [\\n                        {\\n                            \"type\": \"text\",\\n                            \"text\": comment_body\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    }\\n\\n    response = requests.post(url, json=payload ,auth=auth)\\n\\n    if not response.ok:\\n        print(f\"Error posting comment to {issue_key}: {response.status_code} - {response.text}\")\\n\\n    return response.status_code\\n',\n",
       " './cloned_repo\\\\summarizer\\\\summarizer.py': 'import os\\nimport requests\\nfrom config import gemini_api\\n# Set your API key here or via environment variable\\nGEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", gemini_api)\\n\\nGEMINI_API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\\n\\ndef generate_summary(text, instruction=\"Summarize the following text clearly and concisely into two points  by removing unwanted discussion or random talk and remove headings like **Summary** ,**Updates** etc..\"):\\n    if not text.strip():\\n        return \"Input text is empty.\"\\n\\n    headers = {\\n        \"Content-Type\": \"application/json\"\\n    }\\n\\n    payload = {\\n        \"contents\": [{\\n            \"parts\": [\\n                {\"text\": f\"{instruction}\\\\n\\\\n{text}\"}\\n            ]\\n        }],\\n        \"generationConfig\": {\\n            \"temperature\": 0.5,\\n            \"topK\": 40,\\n            \"topP\": 0.95,\\n            \"maxOutputTokens\": 150\\n        }\\n    }\\n\\n    response = requests.post(\\n        f\"{GEMINI_API_URL}?key={GEMINI_API_KEY}\",\\n        headers=headers,\\n        json=payload\\n    )\\n\\n    if response.status_code == 200:\\n        data = response.json()\\n        return data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\\n    else:\\n        return f\"Error: {response.status_code} - {response.text}\"\\n',\n",
       " './cloned_repo\\\\transcript_processor\\\\transcript_processor.py': 'import re\\nfrom collections import defaultdict\\n\\ndef clean_comment(comment, story_id):\\n    # Remove phrases like \\'regarding SCRUM-3\\', \\'as far as SCRUM-4 is concerned\\'\\n    pattern = re.compile(r\"(regarding|as far as|about|concerning|coming to)?\\\\s*{}\\\\b[:,]?\\\\s*\".format(re.escape(story_id)), re.IGNORECASE)\\n    return pattern.sub(\"\", comment).strip()\\n\\ndef extract_updates(conversation_text):\\n    updates = {}\\n    last_story_by_person = {}\\n    story_keywords = defaultdict(set)\\n    ignore_people = {\\'PersonA\\', \\'PersonB\\'} # extra ppl apart from jira stories/ excluded from agile\\n\\n    keyword_memory = defaultdict(lambda: defaultdict(set))  # person -> story -> set of keywords\\n\\n    lines = conversation_text.split(\"\\\\n\")\\n    for line in lines:\\n        line = line.strip()\\n        if not line:\\n            continue\\n\\n        person_match = re.match(r\"^([^:]+):\", line)\\n        if not person_match:\\n            continue\\n\\n        person = person_match.group(1).strip()\\n        if person.lower() in [p.lower() for p in ignore_people]:\\n            continue\\n        if person.lower() in [p.lower() for p in ignore_people]:\\n            if person not in updates:\\n                updates[person] = {}\\n            continue\\n\\n        if person not in updates:\\n            updates[person] = {}\\n\\n        comment_text = line[len(person)+1:].strip()\\n        story_match = re.search(r\"\\\\b(SCRUM-\\\\d+)\\\\b\", line, re.IGNORECASE) #searching a story based on its num /label\\n        matched_story = None\\n\\n        if story_match:\\n            matched_story = story_match.group(1).upper()\\n            last_story_by_person[person] = matched_story\\n            cleaned_comment = clean_comment(comment_text, matched_story)\\n            updates[person].setdefault(matched_story, []).append(cleaned_comment)\\n\\n            # Record keywords for this story and person\\n            for word in cleaned_comment.split():\\n                keyword_memory[person][matched_story].add(word.lower())\\n                story_keywords[matched_story].add(word.lower())\\n        else:\\n            # No explicit story mentioned, infer based on keywords\\n            words = set(comment_text.lower().split())\\n            best_story = None\\n            max_overlap = 0\\n            for story, keywords in keyword_memory[person].items():\\n                overlap = len(words & keywords)\\n                if overlap > max_overlap:\\n                    max_overlap = overlap\\n                    best_story = story\\n\\n            matched_story = best_story or last_story_by_person.get(person)\\n            if matched_story:\\n                cleaned_comment = comment_text.strip()\\n                updates[person].setdefault(matched_story, []).append(cleaned_comment)\\n                for word in cleaned_comment.split():\\n                    keyword_memory[person][matched_story].add(word.lower())\\n                    story_keywords[matched_story].add(word.lower())\\n\\n    # Convert defaultdicts to normal dicts\\n    final_output = {}\\n    for person, stories in updates.items():\\n        final_output[person] = {story: comments for story, comments in stories.items()}\\n\\n    return final_output\\n'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read code from each file and store in dict\n",
    "def read_code_from_files(file_paths):\n",
    "    content={}\n",
    "    for path in file_paths:\n",
    "        with open(path,\"r\",encoding=\"utf-8\",errors=\"ignore\") as f:\n",
    "            content[path]=f.read()\n",
    "    return content\n",
    "read_code_from_files(filtered_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ff65a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree Hierarchical File Chunking tree->folder->file->chunks\n",
    "# Smallest unit = chunk of code (usually a function, class, or block of lines).\n",
    "class CodeChunk:\n",
    "    def __init__(self,text,start_line,end_line,summary=\"\"):\n",
    "        self.text=text  #code text\n",
    "        self.start_line=start_line\n",
    "        self.end_line=end_line\n",
    "        self.summary=summary #summary of the chunk\n",
    "\n",
    "        def to_dict(self):\n",
    "            return {\n",
    "                \"start_line\": self.start_line,\n",
    "                \"end_line\": self.end_line,\n",
    "                \"summary\": self.summary,\n",
    "                \"content\": self.content\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c55353d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileNode:\n",
    "    def __init__(self,path):\n",
    "        self.path=path #file path\n",
    "        self.name = os.path.basename(path)  #file name\n",
    "        self.chunks=[]  #list of CodeChunk\n",
    "        self.summary=None #summary of the file\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"path\": self.path,\n",
    "            \"summary\": self.summary,\n",
    "            \"chunks\": [chunk.to_dict() for chunk in self.chunks]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c4c3bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FolderNode:\n",
    "    def __init__(self,name):\n",
    "        self.name=name\n",
    "        self.files={}   #child files\n",
    "        self.folders={}  #child folders\n",
    "        self.summary=\"\" #summary of the folder\n",
    "\n",
    "    def add_file(self, file_node: FileNode):\n",
    "        self.files[file_node.name] = file_node\n",
    "\n",
    "    def add_folder(self, folder_node: \"FolderNode\"):\n",
    "        self.folders[folder_node.name] = folder_node\n",
    "        \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"summary\": self.summary,\n",
    "            \"files\": {k: v.to_dict() for k, v in self.files.items()},\n",
    "            \"folders\": {k: v.to_dict() for k, v in self.folders.items()},\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8064481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "831ce5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_python_code(content: str, max_lines=200):\n",
    "    chunks = []\n",
    "    try:\n",
    "        tree = ast.parse(content)   # ✅ parse Python code into AST\n",
    "    except Exception:\n",
    "        return [CodeChunk(content, 1, len(content.splitlines()))]\n",
    "\n",
    "    for node in ast.walk(tree):     # ✅ walk all nodes in AST\n",
    "        if isinstance(node, (ast.FunctionDef, ast.ClassDef)):   # ✅ detect function or class\n",
    "            start_line = node.lineno\n",
    "            end_line = getattr(node, \"end_lineno\", start_line + 1)\n",
    "\n",
    "            lines = content.splitlines()[start_line - 1:end_line]   # ✅ extract code text\n",
    "            text = \"\\n\".join(lines)\n",
    "\n",
    "            summary = f\"{'Class' if isinstance(node, ast.ClassDef) else 'Function'} `{node.name}`\"\n",
    "\n",
    "            chunks.append(CodeChunk(text, start_line, end_line, summary))\n",
    "\n",
    "    if not chunks:  # fallback to line-based chunking\n",
    "        lines = content.splitlines()\n",
    "        for i in range(0, len(lines), max_lines):\n",
    "            block = \"\\n\".join(lines[i:i+max_lines])\n",
    "            chunks.append(CodeChunk(block, i+1, min(i+max_lines, len(lines))))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e49a4cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 4. TREE BUILDER =========\n",
    "def build_repo_tree(repo_dict):\n",
    "    root = FolderNode(\"root\")\n",
    "    for path, content in repo_dict.items():\n",
    "        parts = path.split(os.sep)\n",
    "        curr = root\n",
    "\n",
    "        # Create nested folder nodes\n",
    "        for p in parts[:-1]:\n",
    "            if p not in curr.folders:\n",
    "                curr.folders[p] = FolderNode(p)\n",
    "            curr = curr.folders[p]\n",
    "\n",
    "        # Create file node\n",
    "        file_node = FileNode(path)\n",
    "        if path.endswith(\".py\"):\n",
    "            file_node.chunks = chunk_python_code(content)\n",
    "        else:\n",
    "            file_node.chunks = chunk_other_code(content)\n",
    "\n",
    "        curr.files[path] = file_node\n",
    "\n",
    "    return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a72c60a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 5. LLM SUMMARIZER =========\n",
    "def llm_summary(text: str, level=\"chunk\") -> str:\n",
    "    prompt = f\"Summarize this {level} of code in one short, clear sentence:\\n\\n{text[:1500]}\"\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,\n",
    "            max_tokens=80,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"(LLM summary failed: {e})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a10421e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 6. HIERARCHICAL SUMMARIZATION =========\n",
    "def summarize_tree(node: FolderNode):\n",
    "    # Summarize files\n",
    "    for f in node.files.values():\n",
    "        for c in f.chunks:\n",
    "            if not c.summary or c.summary.startswith((\"Class\", \"Function\", \"Code block\")):\n",
    "                c.summary = llm_summary(c.text, level=\"chunk\")\n",
    "        f.summary = llm_summary(\" \".join([c.summary for c in f.chunks]), level=\"file\")\n",
    "\n",
    "    # Recurse into subfolders\n",
    "    for folder in node.folders.values():\n",
    "        summarize_tree(folder)\n",
    "\n",
    "    # Summarize folder itself\n",
    "    child_summaries = [f.summary for f in node.files.values()] + [fld.summary for fld in node.folders.values()]\n",
    "    node.summary = llm_summary(\" \".join(child_summaries), level=\"folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7099a2a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CodeChunk' object has no attribute 'to_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Step 4: Save tree to JSON\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_tree.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 15\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(repo_tree\u001b[38;5;241m.\u001b[39mto_dict(), f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo tree with LLM summaries written to repo_tree.json ✅\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[57], line 19\u001b[0m, in \u001b[0;36mFolderNode.to_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummary,\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m: {k: v\u001b[38;5;241m.\u001b[39mto_dict() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m---> 19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfolders\u001b[39m\u001b[38;5;124m\"\u001b[39m: {k: v\u001b[38;5;241m.\u001b[39mto_dict() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfolders\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m     20\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[57], line 19\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummary,\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m: {k: v\u001b[38;5;241m.\u001b[39mto_dict() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m---> 19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfolders\u001b[39m\u001b[38;5;124m\"\u001b[39m: {k: v\u001b[38;5;241m.\u001b[39mto_dict() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfolders\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m     20\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[57], line 18\u001b[0m, in \u001b[0;36mFolderNode.to_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummary,\n\u001b[1;32m---> 18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m: {k: v\u001b[38;5;241m.\u001b[39mto_dict() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfolders\u001b[39m\u001b[38;5;124m\"\u001b[39m: {k: v\u001b[38;5;241m.\u001b[39mto_dict() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfolders\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m     20\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[57], line 18\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummary,\n\u001b[1;32m---> 18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m: {k: v\u001b[38;5;241m.\u001b[39mto_dict() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfolders\u001b[39m\u001b[38;5;124m\"\u001b[39m: {k: v\u001b[38;5;241m.\u001b[39mto_dict() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfolders\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m     20\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[56], line 13\u001b[0m, in \u001b[0;36mFileNode.to_dict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath,\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummary,\n\u001b[1;32m---> 13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m\"\u001b[39m: [chunk\u001b[38;5;241m.\u001b[39mto_dict() \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunks]\n\u001b[0;32m     14\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[56], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath,\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummary,\n\u001b[1;32m---> 13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m\"\u001b[39m: [chunk\u001b[38;5;241m.\u001b[39mto_dict() \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunks]\n\u001b[0;32m     14\u001b[0m     }\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CodeChunk' object has no attribute 'to_dict'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# ========= 7. EXTENDED PIPELINE =========\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Read repo content\n",
    "    repo_content = read_code_from_files(filtered_paths)\n",
    "\n",
    "    # Step 2: Build tree\n",
    "    repo_tree = build_repo_tree(repo_content)\n",
    "\n",
    "    # Step 3: Generate hierarchical summaries\n",
    "    summarize_tree(repo_tree)\n",
    "\n",
    "    # Step 4: Save tree to JSON\n",
    "    with open(\"repo_tree.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(repo_tree.to_dict(), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"Repo tree with LLM summaries written to repo_tree.json ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b2602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc202f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
